---
title: "Vulnerability Metrics That Matter (and the Ones That Don’t)"
categories:
  - VM
tags:
  - Metrics
excerpt: "Vulnerability programs get measured to death, but most teams aren’t measuring the right things. If a metric doesn’t drive action, it’s probably causing more harm than good. This post breaks down the vulnerability metrics that actually move conversations forward and the common ones that often mislead or distract."
---

I started my security career in vulnerability management and the one thing I learned early on is that vulnerability programs get measured to death, but most people don’t understand what to measure.

**If a metric doesn’t drive action, then it’s probably causing more harm than good.**

The goal of vulnerability metrics are to answer a few basic questions:
- Do we know what we own?
- Do we know what we’re scanning?
- Are we reducing exposure?
- Can we meet our SLAs?

Below are vulnerability metrics that actually drive conversations forward and a few common ones that often waste time or confuse stakeholders.

## The metrics that matter
### Coverage
This is the most foundational metrics in a vulnerability program and one I don’t see emphasized enough.

**Why it matters:** 
If you can’t prove what’s included in scanning, every other metric may be misleading. Mean time to remediate doesn’t mean much if it only reflects a fraction of your environment.

**What to measure:**
- % of assets scanned with authentication
- % of assets scanned within the last X days
- Coverage by environment (workstations, servers, cloud, internal vs. external)
- Coverage gaps by team

### Time to remediate (TTR)
TTR is useful, but it gets misused and misinterpreted when it’s averaged into one number.

**Why is matters:**
A lot can hide behind a single average. Fast closes on low severity findings can make things look great while the growing backlog stays open forever.

**What to measure:**
- TTR by severity and asset criticality (if you have it)
- TTR for internal vs. external
- TTR for exploited vulnerabilities

### SLA adherence
This is the metric that measures execution.

**Why it matters:**
Leadership needs to know whether or not the organization is doing what it said it would do. Done right, this becomes accountability instead of blame and highlights where teams may need more resources.

**What to measure:**
- % of vulnerabilities closed within the SLA broken down by severity
- SLA adherence by team
- Vulnerability exception rate and age 

### Backlog weighted by exposure
Remediating vulnerabilities doesn’t always mean you’re remediating the most risk. You need to count how bad the vulnerabilities are not just how many there are.

**Why it matters:**
A backlog of 7,000 low or medium findings might be mostly noise, but 10 of those vulnerabilities could be on your crown jewels causing real risk.

**What to measure:**
- Days past SLA separated by age buckets (0–30 / 31–60 / 61–90 / 90+)
- Weight by external exposure, asset criticality, exploit maturity

### Reopen rate
Vulnerabilities that continuously reopen can indicate other issues.

**Why it matters:**
If the same vulnerability types or the same hosts keep reappearing, the root issue is not being addressed.

**What to measure:**
- % of closed vulnerabilities that reopen within 30/60/90 days
- Top recurring vulnerability categories (missing patches, misconfigurations)

### Remediation capacity
This measures whether teams are able to consistently move remediate through the workflow.

**Why it matters:**
Even with perfect prioritization, if your workflow can’t handle the volume of findings, the backlog will grow forever.

**What to measure:**
- Findings closed per week and month by team
- Ratio of open vs. closed findings over time
- Time from detection > triage > assignment > remediation > validation

## The metrics that usually don’t matter
### Total vulnerability count
I’ve been guilty of tracking this over the years, and it almost always causes more questions than answers.

**Why it doesn’t matter:**
- Counts move when assets are added/removed, scanning improves, and vulnerability definitions change
- Counts are often inflated by low risk findings
- Counts only measure how many and not how risky
Only when paired with coverage, age buckets, and exposure/criticality context. Without that, it doesn’t measure what most people think it measures.

### Average CVSS score across the environment
I rarely recommend averaging anything in vulnerability metrics. And if you’re still only using CVSS, you’re missing important context.

**Why it doesn’t matter:** 
- CVSS scoring often doesn’t reflect your specific environment
- CVSS ignores context
- Averages hide important details
Use CVSS as one input, then add it with context: external, exploitation, asset criticality, and days past SLA. That’s where it starts to drive decisions.

## Sanity check

A simple way to check any metric is to ask:
- Does this number change any decisions?
- What action do we take if it goes up or down?
- Is this number accurate given our coverage?

If you can’t answer those clearly, the metric is probably noise. If you're feeling overhwelmed, start where you are. Measure what you know and improve the data over time. Simple metrics are better than pretty metrics that drive confusion.